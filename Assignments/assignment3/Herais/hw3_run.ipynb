{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw3_run.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1jPefQqwprZtPHsSxzPm1E6mO-K0oXPBq","authorship_tag":"ABX9TyPg8OjZpOwWsxLLsmqsbUPr"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"EejTibZB-EOz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":205},"outputId":"e31394cc-5480-4b84-cb3f-5ef0704563a5","executionInfo":{"status":"error","timestamp":1588053400432,"user_tz":420,"elapsed":21006,"user":{"displayName":"Xu Vee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNTzTx2PmIBbypp7-eWIRwMo4-I9DLyTz70KE9_g=s64","userId":"12955926615284697296"}}},"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","CS224N 2019-20: Homework 3\n","run.py: Run the dependency parser.\n","Sahil Chopra <schopra8@stanford.edu>\n","Haoshen Hong <haoshen@stanford.edu>\n","\"\"\"\n","from datetime import datetime\n","import os\n","import pickle\n","import math\n","import time\n","import argparse\n","\n","from torch import nn, optim\n","import torch\n","from tqdm import tqdm\n","\n","from parser_model import ParserModel\n","from utils.parser_utils import minibatches, load_and_preprocess_data, AverageMeter\n","\n","parser = argparse.ArgumentParser(description='Train neural dependency parser in pytorch')\n","parser.add_argument('-d', '--debug', action='store_true', help='whether to enter debug mode')\n","args = parser.parse_args()\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["usage: ipykernel_launcher.py [-h] [-d]\n","ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-1c80e27d-cd9f-4585-86e8-3828fa809fee.json\n"],"name":"stderr"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"L3tIA7Y3J55A","colab_type":"code","colab":{}},"source":["# -----------------\n","# Primary Functions\n","# -----------------\n","def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n","    \"\"\" Train the neural dependency parser.\n","\n","    @param parser (Parser): Neural Dependency Parser\n","    @param train_data ():\n","    @param dev_data ():\n","    @param output_path (str): Path to which model weights and results are written.\n","    @param batch_size (int): Number of examples in a single batch\n","    @param n_epochs (int): Number of training epochs\n","    @param lr (float): Learning rate\n","    \"\"\"\n","    best_dev_UAS = 0\n","\n","\n","    ### YOUR CODE HERE (~2-7 lines)\n","    ### TODO:\n","    ###      1) Construct Adam Optimizer in variable `optimizer`\n","    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func` with `mean`\n","    ###         reduction (default)\n","    ###\n","    ### Hint: Use `parser.model.parameters()` to pass optimizer\n","    ###       necessary parameters to tune.\n","    ### Please see the following docs for support:\n","    ###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html\n","    ###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n","\n","    optimizer = optim.Adam(parser.model.parameters(), lr=lr)\n","    loss_func = nn.CrossEntropyLoss(reduction='mean') # reduction = 'mean' as default\n","\n","    ### END YOUR CODE\n","\n","    for epoch in range(n_epochs):\n","        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n","        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)\n","        if dev_UAS > best_dev_UAS:\n","            best_dev_UAS = dev_UAS\n","            print(\"New best dev UAS! Saving model.\")\n","            torch.save(parser.model.state_dict(), output_path)\n","        print(\"\")\n","\n","\n","def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n","    \"\"\" Train the neural dependency parser for single epoch.\n","\n","    Note: In PyTorch we can signify train versus test and automatically have\n","    the Dropout Layer applied and removed, accordingly, by specifying\n","    whether we are training, `model.train()`, or evaluating, `model.eval()`\n","\n","    @param parser (Parser): Neural Dependency Parser\n","    @param train_data ():\n","    @param dev_data ():\n","    @param optimizer (nn.Optimizer): Adam Optimizer\n","    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n","    @param batch_size (int): batch size\n","\n","    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n","    \"\"\"\n","    parser.model.train() # Places model in \"train\" mode, i.e. apply dropout layer\n","    n_minibatches = math.ceil(len(train_data) / batch_size)\n","    loss_meter = AverageMeter()\n","\n","    with tqdm(total=(n_minibatches)) as prog:\n","        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n","            optimizer.zero_grad()   # remove any baggage in the optimizer\n","            loss = 0. # store loss for this batch here\n","            train_x = torch.from_numpy(train_x).long()\n","            train_y = torch.from_numpy(train_y.nonzero()[1]).long()\n","\n","            ### YOUR CODE HERE (~5-10 lines)\n","            ### TODO:\n","            ###      1) Run train_x forward through model to produce `logits`\n","            ###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.\n","            ###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss\n","            ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n","            ###         are the predictions (y^ from the PDF).\n","            ###      3) Backprop losses\n","            ###      4) Take step with the optimizer\n","            ### Please see the following docs for support:\n","            ###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step\n","\n","            logits = parser.model.forward(train_x)\n","            loss = loss_func(logits, train_y)\n","            loss.backward()\n","            optimizer.step()\n","\n","\n","            ### END YOUR CODE\n","            prog.update(1)\n","            loss_meter.update(loss.item())\n","\n","    print (\"Average Train Loss: {}\".format(loss_meter.avg))\n","\n","    print(\"Evaluating on dev set\",)\n","    parser.model.eval() # Places model in \"eval\" mode, i.e. don't apply dropout layer\n","    dev_UAS, _ = parser.parse(dev_data)\n","    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n","    return dev_UAS\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"leIxnGp2Whzy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4705bb76-1ea7-477c-8e62-97919af07869","executionInfo":{"status":"ok","timestamp":1588053453029,"user_tz":420,"elapsed":43401,"user":{"displayName":"Xu Vee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNTzTx2PmIBbypp7-eWIRwMo4-I9DLyTz70KE9_g=s64","userId":"12955926615284697296"}}},"source":["%cd /content/drive/My Drive/cs224n/assignment3\n","!python run.py -d"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/cs224n/assignment3\n","================================================================================\n","INITIALIZING\n","================================================================================\n","Loading data...\n","took 1.98 seconds\n","Building parser...\n","took 0.03 seconds\n","Loading pretrained embeddings...\n","took 2.29 seconds\n","Vectorizing data...\n","took 0.05 seconds\n","Preprocessing training data...\n","took 1.22 seconds\n","took 0.00 seconds\n","\n","================================================================================\n","TRAINING\n","================================================================================\n","Epoch 1 out of 10\n","100% 48/48 [00:02<00:00, 16.59it/s]\n","Average Train Loss: 0.7357395738363266\n","Evaluating on dev set\n","125250it [00:00, 13823555.40it/s]\n","- dev UAS: 50.16\n","New best dev UAS! Saving model.\n","\n","Epoch 2 out of 10\n","100% 48/48 [00:02<00:00, 18.63it/s]\n","Average Train Loss: 0.37260768314202625\n","Evaluating on dev set\n","125250it [00:00, 13730699.84it/s]\n","- dev UAS: 58.49\n","New best dev UAS! Saving model.\n","\n","Epoch 3 out of 10\n","100% 48/48 [00:02<00:00, 18.85it/s]\n","Average Train Loss: 0.31117786280810833\n","Evaluating on dev set\n","125250it [00:00, 11407464.95it/s]\n","- dev UAS: 61.29\n","New best dev UAS! Saving model.\n","\n","Epoch 4 out of 10\n","100% 48/48 [00:02<00:00, 18.67it/s]\n","Average Train Loss: 0.26819199261566\n","Evaluating on dev set\n","125250it [00:00, 13768846.67it/s]\n","- dev UAS: 62.30\n","New best dev UAS! Saving model.\n","\n","Epoch 5 out of 10\n","100% 48/48 [00:02<00:00, 18.66it/s]\n","Average Train Loss: 0.24287904178102812\n","Evaluating on dev set\n","125250it [00:00, 13905150.24it/s]\n","- dev UAS: 65.38\n","New best dev UAS! Saving model.\n","\n","Epoch 6 out of 10\n","100% 48/48 [00:02<00:00, 18.70it/s]\n","Average Train Loss: 0.2197143134350578\n","Evaluating on dev set\n","125250it [00:00, 13738240.44it/s]\n","- dev UAS: 67.97\n","New best dev UAS! Saving model.\n","\n","Epoch 7 out of 10\n","100% 48/48 [00:02<00:00, 18.77it/s]\n","Average Train Loss: 0.20166392407069603\n","Evaluating on dev set\n","125250it [00:00, 13654326.97it/s]\n","- dev UAS: 69.59\n","New best dev UAS! Saving model.\n","\n","Epoch 8 out of 10\n","100% 48/48 [00:02<00:00, 18.59it/s]\n","Average Train Loss: 0.184804513429602\n","Evaluating on dev set\n","125250it [00:00, 13835933.95it/s]\n","- dev UAS: 70.17\n","New best dev UAS! Saving model.\n","\n","Epoch 9 out of 10\n","100% 48/48 [00:02<00:00, 18.92it/s]\n","Average Train Loss: 0.1715450935686628\n","Evaluating on dev set\n","125250it [00:00, 13779320.03it/s]\n","- dev UAS: 71.26\n","New best dev UAS! Saving model.\n","\n","Epoch 10 out of 10\n","100% 48/48 [00:02<00:00, 18.44it/s]\n","Average Train Loss: 0.1597661112124721\n","Evaluating on dev set\n","125250it [00:00, 13510005.81it/s]\n","- dev UAS: 71.82\n","New best dev UAS! Saving model.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vw-VZH36YO04","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4fc46de9-b7f4-4dfd-9bfd-cf4c08af9a03","executionInfo":{"status":"ok","timestamp":1588055067645,"user_tz":420,"elapsed":1320992,"user":{"displayName":"Xu Vee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNTzTx2PmIBbypp7-eWIRwMo4-I9DLyTz70KE9_g=s64","userId":"12955926615284697296"}}},"source":["%cd /content/drive/My Drive/cs224n/assignment3\n","!python run.py"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/cs224n/assignment3\n","================================================================================\n","INITIALIZING\n","================================================================================\n","Loading data...\n","took 1.92 seconds\n","Building parser...\n","took 1.40 seconds\n","Loading pretrained embeddings...\n","took 2.73 seconds\n","Vectorizing data...\n","took 1.76 seconds\n","Preprocessing training data...\n","took 42.54 seconds\n","took 0.01 seconds\n","\n","================================================================================\n","TRAINING\n","================================================================================\n","Epoch 1 out of 10\n","100% 1848/1848 [02:05<00:00, 14.68it/s]\n","Average Train Loss: 0.18117778356795833\n","Evaluating on dev set\n","1445850it [00:00, 48063265.32it/s]\n","- dev UAS: 83.86\n","New best dev UAS! Saving model.\n","\n","Epoch 2 out of 10\n","100% 1848/1848 [02:05<00:00, 14.78it/s]\n","Average Train Loss: 0.11343337228174978\n","Evaluating on dev set\n","1445850it [00:00, 40205087.93it/s]\n","- dev UAS: 86.01\n","New best dev UAS! Saving model.\n","\n","Epoch 3 out of 10\n","100% 1848/1848 [02:01<00:00, 15.21it/s]\n","Average Train Loss: 0.09942459968398466\n","Evaluating on dev set\n","1445850it [00:00, 48022920.80it/s]\n","- dev UAS: 87.14\n","New best dev UAS! Saving model.\n","\n","Epoch 4 out of 10\n","100% 1848/1848 [02:04<00:00, 14.88it/s]\n","Average Train Loss: 0.09117837147222775\n","Evaluating on dev set\n","1445850it [00:00, 47971636.58it/s]\n","- dev UAS: 87.70\n","New best dev UAS! Saving model.\n","\n","Epoch 5 out of 10\n","100% 1848/1848 [02:01<00:00, 15.19it/s]\n","Average Train Loss: 0.0849127983472822\n","Evaluating on dev set\n","1445850it [00:00, 42597387.25it/s]\n","- dev UAS: 87.75\n","New best dev UAS! Saving model.\n","\n","Epoch 6 out of 10\n","100% 1848/1848 [01:59<00:00, 15.43it/s]\n","Average Train Loss: 0.07990594217110247\n","Evaluating on dev set\n","1445850it [00:00, 48890151.87it/s]\n","- dev UAS: 88.13\n","New best dev UAS! Saving model.\n","\n","Epoch 7 out of 10\n","100% 1848/1848 [02:00<00:00, 15.27it/s]\n","Average Train Loss: 0.07594171048492773\n","Evaluating on dev set\n","1445850it [00:00, 48294452.80it/s]\n","- dev UAS: 88.22\n","New best dev UAS! Saving model.\n","\n","Epoch 8 out of 10\n","100% 1848/1848 [02:02<00:00, 15.05it/s]\n","Average Train Loss: 0.07181286959993569\n","Evaluating on dev set\n","1445850it [00:00, 48042703.98it/s]\n","- dev UAS: 88.45\n","New best dev UAS! Saving model.\n","\n","Epoch 9 out of 10\n","100% 1848/1848 [02:03<00:00, 14.94it/s]\n","Average Train Loss: 0.06900531884073179\n","Evaluating on dev set\n","1445850it [00:00, 46998685.89it/s]\n","- dev UAS: 88.73\n","New best dev UAS! Saving model.\n","\n","Epoch 10 out of 10\n","100% 1848/1848 [02:03<00:00, 14.99it/s]\n","Average Train Loss: 0.06596507341500162\n","Evaluating on dev set\n","1445850it [00:00, 48122763.72it/s]\n","- dev UAS: 88.66\n","\n","================================================================================\n","TESTING\n","================================================================================\n","Restoring the best model weights found on the dev set\n","Final evaluation on test set\n","2919736it [00:00, 68738972.49it/s]\n","- test UAS: 89.11\n","Done!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_K1HNIXbWgY2","colab_type":"code","colab":{}},"source":["if __name__ == \"__main__\":\n","    debug = args.debug\n","\n","    assert (torch.__version__.split(\".\") >= [\"1\", \"0\", \"0\"]), \"Please install torch version >= 1.0.0\"\n","\n","    print(80 * \"=\")\n","    print(\"INITIALIZING\")\n","    print(80 * \"=\")\n","    parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)\n","\n","    start = time.time()\n","    model = ParserModel(embeddings)\n","    parser.model = model\n","    print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n","\n","    print(80 * \"=\")\n","    print(\"TRAINING\")\n","    print(80 * \"=\")\n","    output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n","    output_path = output_dir + \"model.weights\"\n","\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n","\n","    if not debug:\n","        print(80 * \"=\")\n","        print(\"TESTING\")\n","        print(80 * \"=\")\n","        print(\"Restoring the best model weights found on the dev set\")\n","        parser.model.load_state_dict(torch.load(output_path))\n","        print(\"Final evaluation on test set\",)\n","        parser.model.eval()\n","        UAS, dependencies = parser.parse(test_data)\n","        print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n","        print(\"Done!\")\n"],"execution_count":0,"outputs":[]}]}