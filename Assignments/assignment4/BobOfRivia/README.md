看完Lecture8后立马开始写 #assignment4，发现很难写下去，于是整理了一下seq2seq-attention的架构，然后较轻松地写完了，并进行了train和test。
看来还是需要对模型架构整体有一个很好的认识才能进行编程，这是我这堂课最大的收获。
代码基本没写注释，因为都是根据pdf写的，每一步a4_handout.pdf上都能找到。
./note/Seq2Seq-Attention for MT.jpg
若写得有问题，希望大家能纠正我，谢谢